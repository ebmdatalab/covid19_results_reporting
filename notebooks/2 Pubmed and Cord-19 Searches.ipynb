{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook performs the searches of PubMed and the CORD-19 Database for text representing trial registrations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PubMed Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "parent = str(Path(cwd).parents[0])\n",
    "sys.path.append(parent)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xmltodict\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from xml.etree.ElementTree import tostring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If the archive exists, load it in.\n",
    "try:\n",
    "    from lib.id_searches import zip_load\n",
    "    archive_df = zip_load(parent + '/data/pubmed/pubmed_archive_1July_2020.csv.zip', \n",
    "                  'pubmed_archive_1July_2020.csv', index_col = 0)\n",
    "\n",
    "#If it doesn't exist, you can do a new PubMed search\n",
    "except FileNotFoundError:\n",
    "    from pymed import PubMed\n",
    "    from lib.credentials import email\n",
    "    from lib.id_searches import query, create_pubmed_archive\n",
    "    print('Archive file not found, conduting new PubMed search.')\n",
    "    pubmed = PubMed(tool=\"Pymed\", email=email)\n",
    "    results = pubmed.query(query, max_results=100000)\n",
    "    \n",
    "    print('Transforming results. This may take a few minutes')\n",
    "    results_list = list(results) #This can take a while\n",
    "    \n",
    "    archive_df = create_pubmed_archive(results_list)\n",
    "    archive_df.to_csv(parent + '/data/pubmed/pubmed_archive_1July_2020.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pubmed_data = archive_df.xml_json.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "pubmed_dicts = []\n",
    "for rec in tqdm(pubmed_data):\n",
    "    pm_dict = json.loads(rec)['PubmedArticle']\n",
    "    entry_dict = {}\n",
    "    art_ids = pm_dict['PubmedData']['ArticleIdList']['ArticleId']\n",
    "    entry_dict['source'] = 'PubMed'\n",
    "    entry_dict['pmid'] = pm_dict['MedlineCitation']['PMID']['#text']\n",
    "    entry_dict['doi'] = None\n",
    "    if isinstance(art_ids, list):\n",
    "        for x in art_ids:\n",
    "            if x['@IdType'] == 'doi':\n",
    "                entry_dict['doi'] = x['#text']\n",
    "    elif isinstance(art_ids, dict):\n",
    "        if art_ids['@IdType'] == 'doi':\n",
    "            entry_dict['doi'] = art_ids['#text']\n",
    "    try:\n",
    "        dbs =  pm_dict['MedlineCitation']['Article']['DataBankList']['DataBank']\n",
    "        accession_nums = []\n",
    "        if isinstance(dbs, list):\n",
    "            for x in dbs:\n",
    "                ans = x['AccessionNumberList']['AccessionNumber']\n",
    "                if isinstance(ans, list):\n",
    "                    accession_nums += ans\n",
    "                else:\n",
    "                    accession_nums.append(x)\n",
    "        elif isinstance(dbs, dict):\n",
    "            x = dbs['AccessionNumberList']['AccessionNumber']\n",
    "            if isinstance(x, list):\n",
    "                accession_nums += x\n",
    "            else:\n",
    "                accession_nums.append(x)\n",
    "                \n",
    "        if accession_nums:\n",
    "            entry_dict['accession'] = accession_nums\n",
    "        else:\n",
    "            entry_dict['accession'] = None\n",
    "    except KeyError:\n",
    "        entry_dict['accession'] = None\n",
    "    \n",
    "    try:\n",
    "        entry_dict['abstract'] = str(pm_dict['MedlineCitation']['Article']['Abstract']['AbstractText'])\n",
    "    except KeyError:\n",
    "        entry_dict['abstract'] = None\n",
    "    \n",
    "    try:\n",
    "        entry_dict['pub_types'] = pm_dict['MedlineCitation']['Article']['PublicationTypeList']\n",
    "    except KeyError:\n",
    "        entry_dict['pub_types'] = None\n",
    "    pubmed_dicts.append(entry_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_end_of_cell_marker": 2
   },
   "outputs": [],
   "source": [
    "#Our searching function and lists of our regular expressions\n",
    "from lib.id_searches import search_text, ids_exact, prefixes, registry_names\n",
    "\n",
    "for d in tqdm(pubmed_dicts):\n",
    "    d['abst_id_hits'] = search_text(ids_exact, d['abstract'])\n",
    "    d['reg_prefix_hits'] = search_text(prefixes, d['abstract'])\n",
    "    d['reg_name_hits'] = search_text(registry_names, d['abstract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pubmed_search_results = pd.DataFrame(pubmed_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_order = ['pmid', 'source', 'abst_id_hits', 'reg_prefix_hits', 'reg_name_hits', 'accession', 'pub_types', 'doi']\n",
    "col_rename = ['id', 'source', 'id_hits', 'prefix_hits', 'reg_name_hits', 'accession', 'pub_types', 'doi']\n",
    "\n",
    "final_pubmed = pubmed_search_results[col_order].reset_index(drop=True)\n",
    "final_pubmed.columns = col_rename\n",
    "final_pubmed['pm_id'] = final_pubmed['id']\n",
    "final_pubmed['cord_id'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pubmed.to_csv(parent + '/data/pubmed/pubmed_search_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Searching CORD-10 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = zip_load(parent + '/data/cord_19/metadata.csv.zip', 'metadata.csv', low_memory = False)\n",
    "metadata['publish_time'] = pd.to_datetime(metadata['publish_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting a list of all the filenames for the papers that were published in 2020\n",
    "covid_19_arts = metadata[metadata.publish_time >= pd.Timestamp(2020,1,1)].sha.to_list()\n",
    "covid_19_pmc = metadata[metadata.publish_time >= pd.Timestamp(2020,1,1)].pmcid.to_list()\n",
    "recent_articles = []\n",
    "for c in covid_19_arts:\n",
    "    recent_articles.append(str(c) + '.json')\n",
    "\n",
    "recent_pmcs = []\n",
    "for p in covid_19_pmc:\n",
    "    recent_pmcs.append(str(p) + '.xml.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The CORD-19 document parses are too big to fit in the GitHub repo so you need to download and add locally\n",
    "#All versions of the CORD-19 database can be accessed here: \n",
    "#https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/historical_releases.html\n",
    "path_pre = parent + '/data/cord_19/document_parses/'\n",
    "\n",
    "pdfs = os.listdir(path_pre + 'pdf_json')\n",
    "pmc = os.listdir(path_pre + 'pmc_json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we can only pull out the recent articles from the pdfs\n",
    "overlap_pdf = list(set(recent_articles).intersection(set(pdfs)))\n",
    "\n",
    "overlap_pmc = list(set(recent_pmcs).intersection(set(pmc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These are searches in the CORD-19 database and take roughly an hour to run both.\n",
    "from lib.id_searches import search_text, ids_exact, prefixes, registry_names\n",
    "\n",
    "cord_pdf_list = []\n",
    "\n",
    "for o in tqdm(overlap_pdf):\n",
    "    with open(path_pre + 'pdf_json' + '/' + o, 'r') as x:\n",
    "        doc_dict = {}\n",
    "        c_text = x.read()\n",
    "        a = json.loads(c_text)\n",
    "        doc_dict['file_name'] = a['paper_id']\n",
    "        doc_dict['source'] = 'cord_pdf'\n",
    "        doc_dict['id_hits'] = search_text(ids_exact, c_text)\n",
    "        doc_dict['reg_prefix_hits'] = search_text(prefixes, c_text)\n",
    "        doc_dict['reg_name_hits'] = search_text(registry_names, c_text)\n",
    "    cord_pdf_list.append(doc_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cord_pmc_list = []\n",
    "\n",
    "for o in tqdm(overlap_pmc):\n",
    "    with open(path_pre + 'pmc_json' + '/' + o, 'r') as x:\n",
    "        doc_dict = {}\n",
    "        c_text = x.read()\n",
    "        a = json.loads(c_text)\n",
    "        doc_dict['file_name'] = a['paper_id']\n",
    "        doc_dict['source'] = 'cord_pmc'\n",
    "        doc_dict['id_hits'] = search_text(ids_exact, c_text)\n",
    "        doc_dict['reg_prefix_hits'] = search_text(prefixes, c_text)\n",
    "        doc_dict['reg_name_hits'] = search_text(registry_names, c_text)\n",
    "    cord_pmc_list.append(doc_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cord_pmc_df = pd.DataFrame(cord_pmc_list)\n",
    "\n",
    "final_pmc = cord_pmc_df.merge(metadata, left_on='file_name', right_on='pmcid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cord_pdf_df = pd.DataFrame(cord_pdf_list)\n",
    "\n",
    "final_pdf = cord_pdf_df.merge(metadata, left_on='file_name', right_on='sha', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_order = ['id', 'source', 'id_hits', 'reg_prefix_hits', 'reg_name_hits', 'accession', 'pub_types', 'doi', \n",
    "             'pubmed_id', 'cord_uid']\n",
    "col_names = ['id', 'source', 'id_hits', 'prefix_hits', 'reg_name_hits', 'accession', 'pub_types', 'doi', 'pm_id', 'cord_id']\n",
    "\n",
    "interim_cord = final_pmc.append(final_pdf, ignore_index=True).reset_index(drop=True)\n",
    "\n",
    "interim_cord['id'] = np.where(interim_cord.pubmed_id.isna(), interim_cord.cord_uid, interim_cord.pubmed_id)\n",
    "interim_cord['accession'] = None\n",
    "interim_cord['pub_types'] = None\n",
    "\n",
    "final_cord = interim_cord[col_order].reset_index(drop=True)\n",
    "final_cord.columns = col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_cord.to_csv(parent + '/data/cord_19/cord_19_search_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Data Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dataset = final_cord.append(final_pubmed, ignore_index=True)\n",
    "combined_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to turn the pub_types column into strings:\n",
    "from lib.id_searches import stringify\n",
    "\n",
    "combined_dataset['pub_types'] = combined_dataset.pub_types.apply(stringify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_1 = combined_dataset.id_hits.notnull()\n",
    "\n",
    "filter_2 = combined_dataset.prefix_hits.notnull()\n",
    "\n",
    "filter_3 = combined_dataset.reg_name_hits.notnull()\n",
    "\n",
    "filter_4 = combined_dataset.accession.notnull() & (combined_dataset.accession != '[]')\n",
    "\n",
    "filter_5 = combined_dataset.pub_types.str.contains(re.compile('(?i)Trial'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = combined_dataset[filter_1 | filter_2 | filter_3 | filter_4 | filter_5].sort_values('id').reset_index(drop=True).fillna(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.id_searches import add_lists, make_doi_url, trial_pub_type\n",
    "\n",
    "combined = filtered[['id', 'id_hits', 'prefix_hits', 'reg_name_hits', 'accession']].groupby('id').agg(add_lists).merge(\n",
    "    filtered[['id', 'pub_types']][filtered.pub_types.notnull()], how='left', left_on='id', right_on='id').merge(\n",
    "    filtered[['id', 'doi', 'pm_id']].drop_duplicates(), how='left', left_on='id', right_on='id').merge(\n",
    "    filtered[['id','cord_id']][filtered.cord_id.notnull()].drop_duplicates(), how='left', left_on='id', right_on='id')\n",
    "\n",
    "combined['doi'] = combined.doi.apply(make_doi_url)\n",
    "combined['pub_types'] = combined.pub_types.apply(trial_pub_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = combined.merge(metadata[['cord_uid', 'url', 'title']], how='left', left_on='cord_id', right_on='cord_uid').drop('cord_uid', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do a final dedupe\n",
    "final_deduped = final.drop_duplicates('id').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_deduped.to_csv(parent + '/data/final_auto_15Sept2020.csv')"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "all",
   "notebook_metadata_filter": "all,-language_info",
   "text_representation": {
    "extension": ".py",
    "format_name": "light",
    "format_version": "1.5",
    "jupytext_version": "1.3.3"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
