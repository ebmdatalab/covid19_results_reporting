{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a collection of webscrapers used to get data we need from registries\n",
    "\n",
    "For now, this is only certain registries as others have so few trials they can be easily manually screened.\n",
    "\n",
    "This doesn't really need to be a notebook but for organisation, disply, and if you are running than all at once, ease. Any of the specific scrapers could be copied into a .py file and run that way if preferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "parent = str(Path(cwd).parents[0])\n",
    "sys.path.append(parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from requests import post\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from time import time\n",
    "import csv\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def get_url(url):\n",
    "    response = get(url, verify = False)\n",
    "    html = response.content\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    return soup\n",
    "\n",
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "save_path = parent + '/data/raw_scraping_output/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trials these were run on are those in the ICTRP dataset after the initial inclusions/exclusions were made (observational, pre-2020 trials, trials that are withdrawn/cancelled). This code assumes you read in that dataset to a DataFrame below and work from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(parent + '/data/ictrp_with_exclusions_29Jul2020.csv')\n",
    "df.source_register.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ClinicalTrials.gov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nct_urls = df[df.source_register == 'ClinicalTrials.gov'].web_address.to_list()\n",
    "pubmed_res_str = 'Publications automatically indexed to this study by ClinicalTrials.gov Identifier (NCT Number):'\n",
    "\n",
    "ctgov_list = []\n",
    "\n",
    "\n",
    "for nct in tqdm(nct_urls):\n",
    "    soup = get_url(nct)\n",
    "    trial_dict = {}\n",
    "    \n",
    "    trial_dict['trial_id'] = nct[-11:]\n",
    "    \n",
    "    #Completion Dates\n",
    "    if soup.find('span', {'data-term': \"Primary Completion Date\"}):\n",
    "        trial_dict['pcd'] = soup.find('span', {'data-term': \"Primary Completion Date\"}).find_next('td').text\n",
    "    else:\n",
    "        trial_dict['pcd'] = None\n",
    "    if soup.find('span', {'data-term': \"Study Completion Date\"}):\n",
    "        trial_dict['scd'] = soup.find('span', {'data-term': \"Study Completion Date\"}).find_next('td').text\n",
    "    else:\n",
    "        trial_dict['scd'] = None\n",
    "\n",
    "    #Tabular Results Status\n",
    "    if soup.find('li', {'id':'results'}):\n",
    "        trial_dict['tab_results'] = soup.find('li', {'id':'results'}).text.strip()\n",
    "    else:\n",
    "        trial_dict['tab_results'] = None\n",
    "\n",
    "    #Auto-linked results via PubMed\n",
    "    if soup.find('span', text=pubmed_res_str):\n",
    "        pm_linked = []\n",
    "        for x in soup.find('span', text=pubmed_res_str).find_next('div').find_all('div'):\n",
    "            pm_linked.append(x.text.strip())\n",
    "        trial_dict['linked_pubs'] = pm_linked\n",
    "    else:\n",
    "        trial_dict['linked_pubs'] = None\n",
    "\n",
    "    #Results citations provided by sponsor\n",
    "    if soup.find('span', text='Publications of Results:'):\n",
    "        spon_pubs = []\n",
    "        for x in soup.find('span', text='Publications of Results:').find_next('div').find_all('div'):\n",
    "            spon_pubs.append(x.text.strip())\n",
    "        trial_dict['spon_pubs'] = spon_pubs\n",
    "    else:\n",
    "        trial_dict['spon_pubs'] = None\n",
    "\n",
    "    #Trial Status:\n",
    "    if soup.find('span', {'data-term': 'Recruitment Status'}):\n",
    "        trial_dict['trial_status'] = soup.find('span', {'data-term': 'Recruitment Status'}).next_sibling.replace(':','').strip()\n",
    "    else:\n",
    "        trial_dict['trial_status'] = None\n",
    "\n",
    "    #Secondary IDs:\n",
    "    if soup.find('td', text='Other Study ID Numbers:'):\n",
    "        ids = []\n",
    "        for a in soup.find('td', text='Other Study ID Numbers:').find_next('td').text.split('\\n'):\n",
    "            if a.strip():\n",
    "                ids.append(a.strip())\n",
    "        trial_dict['secondary_ids'] = ids\n",
    "    \n",
    "    ctgov_list.append(trial_dict)\n",
    "    \n",
    "#Can be expanded for some covariates as needed but also can archive our full copy from the FDAAA TT \n",
    "#on the day of the scrape and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctgov_results = pd.DataFrame(ctgov_list)\n",
    "\n",
    "ctgov_results['pcd'] = pd.to_datetime(ctgov_results['pcd'])\n",
    "ctgov_results['scd'] = pd.to_datetime(ctgov_results['scd'])\n",
    "\n",
    "ctgov_results.to_csv(save_path + 'ctgov_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ISRCTN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ISRCTN\n",
    "\n",
    "isrctn_urls = df[df.source_register == 'ISRCTN'].web_address.to_list()\n",
    "\n",
    "isrctn_list = []\n",
    "\n",
    "for i in tqdm(isrctn_urls):\n",
    "\n",
    "    trial_dict = {}\n",
    "    soup = get_url(i)\n",
    "\n",
    "    #Trial ID\n",
    "    trial_dict['trial_id'] = soup.find('span', {'class':'ComplexTitle_primary'}).text\n",
    "\n",
    "    #Trial Status\n",
    "    if soup.find('dt', text='Overall trial status'):\n",
    "        trial_dict['overall_status'] = soup.find('dt', text='Overall trial status').find_next().text.strip()\n",
    "    else:\n",
    "        trial_dict['overall_status'] = None\n",
    "        \n",
    "    trial_dict['recruitment_status'] = soup.find('dt', text='Recruitment status').find_next().text.strip()\n",
    "\n",
    "    #Dates\n",
    "    trial_dict['trial_start_date'] = soup.find('h3', text='Overall trial start date').find_next().text.strip()\n",
    "\n",
    "    #Dates\n",
    "    trial_dict['trial_end_date'] = soup.find('h3', text='Overall trial end date').find_next().text.strip()\n",
    "    \n",
    "    #Other IDs\n",
    "    sid_dict={}\n",
    "    if soup.find('h3', text='EudraCT number').find_next().text.strip():\n",
    "        sid_dict['eudract_number'] = soup.find('h3', text='EudraCT number').find_next().text.strip()\n",
    "    else:\n",
    "        sid_dict['eudract_number'] = None\n",
    "    if soup.find('h3', text='ClinicalTrials.gov number').find_next().text.strip():\n",
    "        sid_dict['ctgov_number'] = soup.find('h3', text='ClinicalTrials.gov number').find_next().text.strip()\n",
    "    else:\n",
    "        sid_dict['ctgov_number'] = None\n",
    "    if soup.find('h3', text='Protocol/serial number').find_next().text.strip():\n",
    "        sid_dict['other_id'] = soup.find('h3', text='Protocol/serial number').find_next().text.strip()\n",
    "    else:\n",
    "        sid_dict['other_id'] = None\n",
    "    trial_dict['secondary_ids'] = sid_dict\n",
    "\n",
    "    #Results stuff\n",
    "    if soup.find('h3', text='Basic results (scientific)').find_next():\n",
    "        trial_dict['basic_results'] = soup.find('h3', text='Basic results (scientific)').find_next().text.strip()\n",
    "    else:\n",
    "        trial_dict['basic_results'] = None\n",
    "    if soup.find('h3', text='Publication list').find_next().text.strip():\n",
    "        trial_dict['pub_list'] = soup.find('h3', text='Publication list').find_next().text.strip()\n",
    "    else:\n",
    "        trial_dict['pub_list'] = None\n",
    "    \n",
    "    isrctn_list.append(trial_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(isrctn_list).to_csv(save_path + 'isrctn_2jul_2020.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EUCTR\n",
    "IDs will be injested in form of EUCTR2020-000890-25-FR\n",
    "We need to kill the EUCTR and the -FR part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euctr_urls = df[df.source_register == 'EU Clinical Trials Register'].web_address.to_list()\n",
    "\n",
    "euctr_ids = df[df.source_register == 'EU Clinical Trials Register'].trialid.to_list()\n",
    "\n",
    "euctr_trials = []\n",
    "\n",
    "for i in tqdm(euctr_ids):\n",
    "\n",
    "    euctr_id = re.search('(.*\\d)', i.replace('EUCTR',''))[0]\n",
    "\n",
    "    search_url = 'https://www.clinicaltrialsregister.eu/ctr-search/search?query={}'\n",
    "    #First blank is the trial number, 2nd is the abbreviation for the protocol country\n",
    "    protocol_url = 'https://www.clinicaltrialsregister.eu/ctr-search/trial/{}/{}'\n",
    "\n",
    "    soup=get_url(search_url.format(euctr_id))\n",
    "\n",
    "    trial_dict = {}\n",
    "\n",
    "    #trial id\n",
    "    trial_dict['trial_id'] = euctr_id\n",
    "\n",
    "    #Results status\n",
    "    trial_dict['results_status'] = soup.find('span', {'class':'label'}, text='Trial results:').find_next().text\n",
    "\n",
    "    #Countries\n",
    "    country_list = []\n",
    "    for x in soup.find('span', text='Trial protocol:').parent.find_all('a'):\n",
    "        country_list.append(x.text)\n",
    "    trial_dict['countries'] = country_list\n",
    "\n",
    "    #Individual Protocol Data\n",
    "    #Completion dates\n",
    "    comp_dates = []\n",
    "    status_list = []\n",
    "    for c in country_list:\n",
    "        psoup = get_url(protocol_url.format(euctr_id, c))\n",
    "        if psoup.find('td', text='Date of the global end of the trial'):\n",
    "            comp_dates.append(psoup.find('td', text='Date of the global end of the trial').find_next().text)\n",
    "        else:\n",
    "            comp_dates.append(None)\n",
    "    \n",
    "    #Trial status\n",
    "        if psoup.find('td', text='Trial Status:'):\n",
    "            status_list.append(psoup.find('td', text='Trial Status:').find_next('td').text.strip())\n",
    "        else:\n",
    "            status_list.append(None)\n",
    "\n",
    "    #secondary_ids\n",
    "        sid_dict = {}\n",
    "        if psoup.find('td', text='ISRCTN (International Standard Randomised Controlled Trial) Number'):\n",
    "            sid_dict['isrctn'] = psoup.find('td', text='ISRCTN (International Standard Randomised Controlled Trial) Number').find_next().text.strip()\n",
    "        if psoup.find('td', text='US NCT (ClinicalTrials.gov registry) number'):\n",
    "            sid_dict['nct_id'] = psoup.find('td', text='US NCT (ClinicalTrials.gov registry) number').find_next().text.strip()\n",
    "        if psoup.find('td', text=\"Sponsor's protocol code number\"):\n",
    "            sid_dict['spon_id'] = psoup.find('td', text=\"Sponsor's protocol code number\").find_next().text.strip()\n",
    "\n",
    "    trial_dict['global_trial_end_dates'] =  comp_dates\n",
    "    trial_dict['status_list'] = status_list\n",
    "    if len(sid_dict) > 0:\n",
    "        trial_dict['secondary_ids'] = sid_dict\n",
    "    else:\n",
    "        trial_dict['secondary_ids'] = None\n",
    "    \n",
    "    euctr_trials.append(trial_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(euctr_trials).to_csv(save_path + 'euctr_1jul_2020.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DRKS\n",
    "We can get DRKS trials via the URL in the ICTRP dataset like:\n",
    "https://www.drks.de/drks_web/navigate.do?navigationId=trial.HTML&TRIAL_ID=DRKS00021186"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def does_it_exist(soup, element, e_class, n_e=False):\n",
    "    if not n_e:\n",
    "        location = soup.find(element, class_=e_class).text.strip()\n",
    "    elif n_e:\n",
    "        location = soup.find(element, class_=e_class).next_element.next_element.next_element.next_element.strip()\n",
    "    if location == '[---]*':\n",
    "        field = None\n",
    "    else:\n",
    "        field = location\n",
    "    return field\n",
    "\n",
    "drks_urls = df[df.source_register == 'German Clinical Trials Register'].web_address.to_list()\n",
    "\n",
    "drks_trials = []\n",
    "\n",
    "for d in tqdm(drks_urls): \n",
    "    \n",
    "    soup = get_url(d)\n",
    "\n",
    "    trial_dict = {}\n",
    "\n",
    "    trial_dict['trial_id'] = soup.find('li', class_='drksId').next_element.next_element.next_element.next_element.strip()\n",
    "\n",
    "    st_class = ['state', 'deadline']\n",
    "    st_labels = ['recruitment_status', 'study_closing_date']\n",
    "    for lab, s_class in zip(st_labels, st_class):\n",
    "        trial_dict[lab] = does_it_exist(soup, 'li', s_class, n_e=True)\n",
    "\n",
    "    s_id_list = []\n",
    "    ul = soup.find('ul', class_='secondaryIDs').find_all('li')\n",
    "    if ul[0].text == '[---]*':\n",
    "        s_id_list = None\n",
    "    else:\n",
    "        for u in ul:\n",
    "            s_id_dict = {}\n",
    "            s_id_dict['id_type'] = u.next_element.next_element.next_element.strip().replace(':','')\n",
    "            li_t = u.next_element.next_element.next_element.next_element.strip()\n",
    "            li_t = re.sub('\\n', '|', li_t)\n",
    "            li_t = re.sub('\\s+', '', li_t).replace('(','').replace(')','')\n",
    "            id_info = li_t.split('|')\n",
    "            if len(id_info) > 1:   \n",
    "                s_id_dict['registry'] = id_info[1]\n",
    "                s_id_dict['secondary_id'] = id_info[0]\n",
    "            else:\n",
    "                s_id_dict['registry'] = None\n",
    "                s_id_dict['secondary_id'] = id_info[0]\n",
    "            s_id_list.append(s_id_dict)\n",
    "\n",
    "    trial_dict['secondary_ids'] = s_id_list\n",
    "\n",
    "    docs_list = []\n",
    "    ul = soup.find('ul', class_='publications').find_all('li')\n",
    "    if ul[0].text == '[---]*':\n",
    "        docs_list = None\n",
    "    else:\n",
    "        for u in ul:\n",
    "            doc_dict = {}\n",
    "            doc_dict['document_type'] = u.next_element.next_element.next_element.strip().replace(':','')\n",
    "            if u.find('a'):\n",
    "                doc_dict['link_to_document'] = u.find('a').get('href')\n",
    "            else:\n",
    "                doc_dict['link_to_document'] = None\n",
    "            docs_list.append(doc_dict)\n",
    "    trial_dict['results_publications_documents'] = docs_list\n",
    "    \n",
    "    drks_trials.append(trial_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(drks_trials).to_csv(save_path + 'drks_trials_1jul_2020.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CTRI\n",
    "\n",
    "We can get right to a trial registration with a URL from the ICTRP like:\n",
    "http://www.ctri.nic.in/Clinicaltrials/pmaindet2.php?trialid=43553"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need a slightly more robust function to fetch trial data from the CTRI\n",
    "def get_ctri(url):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "    tries=3\n",
    "    for i in range(tries):\n",
    "        try:\n",
    "            response = get(url, verify = False, headers=headers)\n",
    "        except ConnectionError as e:\n",
    "            if i < tries - 1:\n",
    "                sleep(2)\n",
    "                continue\n",
    "            else:\n",
    "                raise\n",
    "    html = response.content\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    return soup\n",
    "\n",
    "ctri_urls = df[df.source_register == 'CTRI'].web_address.to_list()\n",
    "\n",
    "ctri_list = []\n",
    "\n",
    "for c in tqdm(ctri_urls):\n",
    "\n",
    "    soup = get_ctri(c)\n",
    "\n",
    "    trial_dict = {}\n",
    "\n",
    "    trial_dict['trial_id'] = soup.find('td', text = re.compile('CTRI Number\\s*')).find_next('b').find_next('b').text.strip()\n",
    "    \n",
    "    trial_dict['completion_date_india'] = soup.find('td', text = 'Date of Study Completion (India)').find_next('td').text.strip()\n",
    "\n",
    "    trial_dict['completion_date_global'] = soup.find('td', text = 'Date of Study Completion (Global)').find_next('td').text.strip()\n",
    "\n",
    "    trial_dict['recruitment_status_india'] = soup.find('b', text='Recruitment Status of Trial (India)').find_next('td').text.strip()\n",
    "\n",
    "    trial_dict['recruitment_status_global'] = soup.find('b', text='Recruitment Status of Trial (Global)').find_next('td').text.strip()\n",
    "\n",
    "    trial_dict['publication_details'] = soup.find('b', text='Publication Details').find_next('td').text.strip()\n",
    "\n",
    "    if soup.find('b', text = re.compile('Secondary IDs if Any')):\n",
    "        trial_dict['secondary_ids'] = soup.find('b', text = re.compile('Secondary IDs if Any')).parent.parent.find_all('tr')\n",
    "    else:\n",
    "        trial_dict['secondary_ids'] = None\n",
    "\n",
    "    ctri_list.append(trial_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctri_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(ctri_list).to_csv(save_path + 'ctri_2jul2020_fix.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANZCTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anzctr_urls = df[df.source_register == 'ANZCTR'].web_address.to_list()\n",
    "\n",
    "anzctr_trials = []\n",
    "\n",
    "for u in tqdm(anzctr_urls):\n",
    "\n",
    "    soup = get_url(u)\n",
    "\n",
    "    trial_dict = {}\n",
    "\n",
    "    trial_dict['trial_id'] = soup.find('span', {'id': 'ctl00_body_CXACTRNUMBER'}).text.replace('p','')\n",
    "\n",
    "    trial_dict['last_updated'] = soup.find('span', {'id': 'ctl00_body_CXUPDATEDATE'}).text\n",
    "\n",
    "    trial_dict['trial_status'] = soup.find('span', {'id': 'ctl00_body_CXRECRUITMENTSTATUS'}).text\n",
    "\n",
    "    anticipated_end_date = soup.find('span', {'id': 'ctl00_body_CXANTICIPATEDENDDATE'}).text\n",
    "\n",
    "    actual_end_date = soup.find('span', {'id': 'ctl00_body_CXACTUALENDDATE'}).text\n",
    "\n",
    "    if anticipated_end_date:\n",
    "        trial_dict['completion_date'] = anticipated_end_date\n",
    "    else:\n",
    "        trial_dict['completion_date'] = actual_end_date\n",
    "\n",
    "    secondary_ids = []\n",
    "\n",
    "    for x in soup.find_all('span', text=re.compile('Secondary ID \\[\\d*\\]')):\n",
    "        secondary_ids.append(x.find_next('div').span.text.strip())\n",
    "\n",
    "    trial_dict['secondary_ids'] = secondary_ids\n",
    "\n",
    "    results_dict = {}\n",
    "\n",
    "    if soup.find('div', {'id': 'ctl00_body_divNoResultsANZCTR'}):\n",
    "        trial_dict['results'] = None\n",
    "    else:\n",
    "        citations = []\n",
    "        for x in soup.find_all('span', text=re.compile('Publication date and citation/details \\[\\d*\\]')):\n",
    "            citations.append(x.find_next('div').span.text)\n",
    "\n",
    "        results_dict['citations'] = citations\n",
    "\n",
    "        if soup.find('a', {'id': 'ctl00_body_hyperlink_CXRESULTATTACHMENT'}):\n",
    "            results_dict['basic_reporting_doc'] = soup.find('a', {'id': 'ctl00_body_hyperlink_CXRESULTATTACHMENT'}).text\n",
    "\n",
    "    trial_dict['results'] = results_dict\n",
    "    \n",
    "    anzctr_trials.append(trial_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anzctr_df = pd.DataFrame(anzctr_trials)\n",
    "anzctr_df.to_csv(save_path + 'anzctr_trials_2jul2020.csv.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Easiest to just to this query and then filter\n",
    "\n",
    "rsp = post(\n",
    "    \"https://api.trialregister.nl/trials/public.trials/_msearch?\",\n",
    "    headers={\"Content-Type\": \"application/x-ndjson\", \"Accept\": \"application/json\"},\n",
    "    data='{\"preference\":\"results\"}\\n{\"query\":{\"match_all\":{}},\"size\":10000,\"_source\":{\"includes\":[\"*\"],\"excludes\":[]},\"sort\":[{\"id\":{\"order\":\"desc\"}}]}\\n',\n",
    ")\n",
    "results = rsp.json()\n",
    "hits = results[\"responses\"][0][\"hits\"][\"hits\"]\n",
    "records = [hit[\"_source\"] for hit in hits]\n",
    "\n",
    "all_keys = set().union(*(record.keys() for record in records))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(all_keys)\n",
    "\n",
    "from datetime import date\n",
    "import csv\n",
    "\n",
    "def ntr_csv(save_path):\n",
    "    with open(save_path + 'ntr - ' + str(date.today()) + '.csv','w', newline = '', encoding='utf-8') as ntr_csv:\n",
    "        writer=csv.DictWriter(ntr_csv,fieldnames=labels)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntr_csv(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IRCT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irct_urls = df[df.source_register == 'IRCT'].web_address.to_list()\n",
    "\n",
    "irct_list = []\n",
    "\n",
    "for url in tqdm(irct_urls):\n",
    "\n",
    "    soup=get_url(url)\n",
    "\n",
    "    trial_dict = {}\n",
    "\n",
    "    trial_dict['trial_id'] = soup.find('span', text='IRCT registration number:').find_next('strong').text.strip()\n",
    "\n",
    "    trial_dict['trial_status'] = soup.find('dt', text=re.compile('\\sRecruitment status\\s')).find_next('dd').text.strip()\n",
    "\n",
    "    if soup.find('dt', text=re.compile('\\sTrial completion date\\s')).find_next('dd').text.strip() == 'empty':\n",
    "        trial_dict['completion_date'] = None\n",
    "    else:\n",
    "        trial_dict['completion_date'] = re.findall(re.compile('\\d{4}-\\d{2}-\\d{2}'), soup.find('dt', text=re.compile('\\sTrial completion date\\s')).find_next('dd').text.strip())[0]\n",
    "\n",
    "    trial_dict['secondary_ids'] = soup.find('h3', text=re.compile('Secondary Ids')).parent\n",
    "    \n",
    "    irct_list.append(trial_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(irct_list).to_csv(save_path + 'irct_1jul_2020.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChiCTR\n",
    "\n",
    "This scraper works poorly. You only get between 10-30 trials scraped before you run into the anti-dos blocks. I ran it multiple times over about a day and a half to gather the data on ChiCTR trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "chictr_urls = df[df.source_register == 'ChiCTR'].web_address.to_list()\n",
    "\n",
    "chictr_trials = []\n",
    "\n",
    "for u in tqdm(chictr_urls[299:]):\n",
    "    \n",
    "    soup=get_url(u)\n",
    "    \n",
    "    trial_dict = {}\n",
    "    \n",
    "    trial_dict['trial_id'] = soup.find('p', text = re.compile('\\w*Registration number\\w*')).find_next('td').text.strip()\n",
    "    \n",
    "    if len(re.findall(re.compile('\\d{4}-\\d{2}-\\d{2}'), soup.find('span', text='To').parent.text)) > 1:\n",
    "         trial_dict['comp_date'] = re.findall(re.compile('\\d{4}-\\d{2}-\\d{2}'), soup.find('span', text='To').parent.text)[1]\n",
    "    else:\n",
    "        trial_dict['comp_date'] = None\n",
    "            \n",
    "\n",
    "    trial_dict['trial_status'] = soup.find('p', text = re.compile('\\w*Recruiting status\\w*')).find_next('p').find_next('p').text.strip()\n",
    "    \n",
    "    chictr_trials.append(trial_dict)\n",
    "    \n",
    "    sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(chictr_trials).to_csv(save_path + 'chictr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "all",
   "notebook_metadata_filter": "all,-language_info",
   "text_representation": {
    "extension": ".py",
    "format_name": "light",
    "format_version": "1.5",
    "jupytext_version": "1.3.3"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
